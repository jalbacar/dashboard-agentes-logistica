# Gu铆a de Instalaci贸n y Configuraci贸n

##  Inicio R谩pido

### 1. Preparaci贸n del Entorno

```bash
# Clonar el repositorio
git clone <tu-repo-url>
cd proyecto_logista_v1

# Copiar configuraci贸n
cp .env.example .env
```

### 2. Elegir Proveedor de IA

#### Opci贸n A: Ollama (Recomendado para desarrollo)
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Descargar modelo
ollama pull llama3.2

# Configurar .env
echo "LLM_PROVIDER=ollama" >> .env
```

#### Opci贸n B: OpenAI (Recomendado para producci贸n)
```bash
# Obtener API Key de https://platform.openai.com/api-keys
# Configurar .env
echo "LLM_PROVIDER=openai" >> .env
echo "OPENAI_API_KEY=tu-api-key-aqui" >> .env
```

### 3. Ejecutar la Aplicaci贸n

```bash
# Construir y ejecutar
docker-compose up -d --build

# Verificar estado
curl http://localhost:8000/llm-status
```

##  Configuraci贸n Avanzada

### Variables de Entorno

| Variable | Descripci贸n | Valores | Default |
|----------|-------------|---------|---------|
| `LLM_PROVIDER` | Proveedor de IA | `openai`, `ollama` | `openai` |
| `OPENAI_API_KEY` | Clave API de OpenAI | `sk-...` | - |
| `OLLAMA_MODEL` | Modelo de Ollama | `llama3.2`, `llama2`, etc. | `llama3.2` |
| `OLLAMA_BASE_URL` | URL de Ollama | URL completa | `http://host.docker.internal:11434` |

### Modelos Ollama Recomendados

```bash
# Modelos ligeros (< 4GB RAM)
ollama pull llama3.2:1b
ollama pull phi3:mini

# Modelos est谩ndar (8GB RAM)
ollama pull llama3.2
ollama pull mistral

# Modelos avanzados (16GB+ RAM)
ollama pull llama3.1:70b
ollama pull codellama:34b
```

##  Troubleshooting

### Problema: "Error conectando con Ollama"

**Soluci贸n:**
```bash
# Verificar que Ollama est茅 ejecut谩ndose
ollama list

# Reiniciar Ollama
ollama serve

# Verificar conectividad desde Docker
docker exec logistics_backend curl http://host.docker.internal:11434/api/tags
```

### Problema: "OpenAI API Key inv谩lida"

**Soluci贸n:**
```bash
# Verificar la clave
curl -H "Authorization: Bearer $OPENAI_API_KEY" https://api.openai.com/v1/models

# Regenerar clave en https://platform.openai.com/api-keys
```

### Problema: "Frontend no se conecta al backend"

**Soluci贸n:**
```bash
# Verificar que ambos contenedores est茅n ejecut谩ndose
docker ps

# Verificar logs
docker logs logistics_backend
docker logs logistics_frontend

# Reiniciar servicios
docker-compose restart
```

### Problema: "Optimizaci贸n muy lenta"

**Soluciones:**
1. **Usar modelo m谩s ligero:**
   ```bash
   # En .env
   OLLAMA_MODEL=llama3.2:1b
   ```

2. **Cambiar a OpenAI:**
   ```bash
   # En .env
   LLM_PROVIDER=openai
   ```

3. **Usar solo algoritmo b谩sico:**
   - Comentar `OPENAI_API_KEY` en `.env`
   - El sistema usar谩 autom谩ticamente el algoritmo b谩sico

##  Monitoreo y Logs

### Verificar Estado del Sistema

```bash
# Estado general
curl http://localhost:8000/health

# Estado de IA
curl http://localhost:8000/llm-status

# Informaci贸n del servidor
curl http://localhost:8000/
```

### Logs en Tiempo Real

```bash
# Backend
docker logs -f logistics_backend

# Frontend
docker logs -f logistics_frontend

# Ambos servicios
docker-compose logs -f
```

### M茅tricas de Rendimiento

```bash
# Tiempo de respuesta
time curl -X POST http://localhost:8000/api/optimize-routes \
  -H "Content-Type: application/json" \
  -d '{"deliveries":[{"id":"d1","weight":10}],"fleet":[{"id":"v1","capacity":20}]}'
```

##  Desarrollo Local

### Sin Docker

```bash
# Backend
cd crewai_backend
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Frontend (nueva terminal)
cd frontend
npm install
npm start
```

### Con Hot Reload

```bash
# Modificar docker-compose.yml para desarrollo
# Agregar vol煤menes para hot reload
```

##  Despliegue en Producci贸n

### Consideraciones

1. **Usar OpenAI en producci贸n** (m谩s estable que Ollama)
2. **Configurar l铆mites de recursos** en Docker
3. **Implementar load balancer** para m煤ltiples instancias
4. **Monitoreo con Prometheus/Grafana**
5. **Logs centralizados con ELK Stack**

### Variables de Producci贸n

```bash
# .env.production
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-prod-key-here
ENVIRONMENT=production
LOG_LEVEL=INFO
```

##  Soporte

Si encuentras problemas:

1. Revisa los logs: `docker-compose logs`
2. Verifica la configuraci贸n: `curl http://localhost:8000/llm-status`
3. Consulta la documentaci贸n de la API: `http://localhost:8000/docs`
4. Abre un issue en el repositorio con los logs relevantes