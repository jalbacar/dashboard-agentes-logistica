# Configuración de LLM Provider
# Opciones: "openai" o "ollama"
LLM_PROVIDER=openai

# Configuración OpenAI (si LLM_PROVIDER=openai)
OPENAI_API_KEY=tu-openai-api-key-aqui

# Configuración Ollama (si LLM_PROVIDER=ollama)
OLLAMA_MODEL=llama3.2
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Para usar OpenAI:
# 1. Ve a https://platform.openai.com/api-keys
# 2. Crea una nueva API key
# 3. Configura LLM_PROVIDER=openai y OPENAI_API_KEY

# Para usar Ollama:
# 1. Instala Ollama: https://ollama.ai
# 2. Ejecuta: ollama pull llama3.2
# 3. Configura LLM_PROVIDER=ollama