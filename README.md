# Dashboard de Agentes de LogÃ­stica

Sistema de optimizaciÃ³n de rutas logÃ­sticas usando agentes de IA con FastAPI y React.

## ğŸš€ CaracterÃ­sticas

- **Backend FastAPI**: API REST para optimizaciÃ³n de rutas con IA
- **Frontend React**: Dashboard interactivo con TypeScript
- **ContainerizaciÃ³n**: Docker y Docker Compose
- **IA Flexible**: Soporte para OpenAI GPT y Ollama (modelos locales)
- **OptimizaciÃ³n Inteligente**: Algoritmo bÃ¡sico + optimizaciÃ³n con LLM
- **Fallback AutomÃ¡tico**: Si falla la IA, usa algoritmo bÃ¡sico

## ğŸ“‹ Requisitos

- Docker
- Docker Compose
- **OpciÃ³n A**: OpenAI API Key (para usar GPT)
- **OpciÃ³n B**: Ollama instalado localmente (modelos gratuitos)

## ğŸ› ï¸ InstalaciÃ³n y Uso

1. **Clonar el repositorio**
```bash
git clone <tu-repo-url>
cd proyecto_logista_v1
```

2. **Configurar variables de entorno**
```bash
cp .env.example .env
# Editar .env segÃºn tu preferencia:
# - Para OpenAI: LLM_PROVIDER=openai y OPENAI_API_KEY
# - Para Ollama: LLM_PROVIDER=ollama (gratuito)
```

3. **Ejecutar con Docker Compose**
```bash
docker-compose up -d --build
```

4. **Acceder a la aplicaciÃ³n**
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- DocumentaciÃ³n API: http://localhost:8000/docs

## ğŸ—ï¸ Estructura del Proyecto

```
proyecto_logista_v1/
â”œâ”€â”€ frontend/                 # AplicaciÃ³n React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx          # Componente principal
â”‚   â”‚   â””â”€â”€ index.tsx        # Punto de entrada
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ crewai_backend/          # API FastAPI
â”‚   â”œâ”€â”€ main.py              # Servidor principal
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ docker-compose.yml       # ConfiguraciÃ³n de contenedores
```

## ğŸ”§ Desarrollo

### Backend
```bash
cd crewai_backend
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend
```bash
cd frontend
npm install
npm start
```

## ğŸ“¡ API Endpoints

- `POST /api/optimize-routes` - Optimizar rutas de entrega con IA
- `GET /health` - Estado del servicio
- `GET /llm-status` - Estado de la configuraciÃ³n de IA
- `GET /` - InformaciÃ³n del servidor

## ğŸ³ Docker

El proyecto incluye configuraciÃ³n completa de Docker:
- **Frontend**: Node.js 18 Alpine con proxy al backend
- **Backend**: Python 3.11 Slim con soporte para Ollama
- **Red**: ComunicaciÃ³n entre contenedores
- **Ollama**: Acceso a host local via `host.docker.internal`

## ğŸ” Monitoreo

```bash
# Verificar estado de la IA
curl http://localhost:8000/llm-status

# Logs del backend
docker logs logistics_backend

# Logs del frontend
docker logs logistics_frontend
```

## ğŸ¤– ConfiguraciÃ³n de IA

### OpciÃ³n 1: OpenAI (Recomendado para producciÃ³n)
```bash
# En .env
LLM_PROVIDER=openai
OPENAI_API_KEY=tu-api-key-aqui
```

### OpciÃ³n 2: Ollama (Gratuito, local)
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Descargar modelo
ollama pull llama3.2

# En .env
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3.2
```

## ğŸ“ Ejemplo de Uso

```json
POST /api/optimize-routes
{
  "deliveries": [
    {"id": "d1", "weight": 10, "orderId": "o1"},
    {"id": "d2", "weight": 25, "orderId": "o2"}
  ],
  "fleet": [
    {"id": "v1", "capacity": 35, "type": "truck"}
  ]
}
```

**Respuesta:**
```json
{
  "optimizedRoutes": [
    {
      "routeId": "RUTA-123",
      "vehicleId": "v1",
      "stops": [{"id": "d1", "weight": 10}, {"id": "d2", "weight": 25}],
      "totalWeight": 35
    }
  ],
  "unassignedDeliveries": [],
  "optimization_method": "llm_openai",
  "llm_used": true,
  "message": "OptimizaciÃ³n realizada con OPENAI"
}
```

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.